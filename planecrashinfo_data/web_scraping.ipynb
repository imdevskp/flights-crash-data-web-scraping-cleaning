{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# ================\n",
    "\n",
    "# for date and time opeations\n",
    "from datetime import datetime\n",
    "# for file and folder operations\n",
    "import os\n",
    "# for regular expression opeations\n",
    "import re\n",
    "# for listing files in a folder\n",
    "import glob\n",
    "# for getting web contents\n",
    "import requests \n",
    "# storing and analysing data\n",
    "import pandas as pd\n",
    "# for scraping web contents\n",
    "from bs4 import BeautifulSoup\n",
    "# to save and load python data\n",
    "import pickle\n",
    "# numerical processing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# ========\n",
    "\n",
    "# link at which web data recides\n",
    "index_page = 'http://www.planecrashinfo.com/database.htm'\n",
    "# get web data\n",
    "req = requests.get(index_page)\n",
    "# parse web data\n",
    "soup = BeautifulSoup(req.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/1920/1920.htm\"> 1920</a>,\n",
       " <a href=\"/1921/1921.htm\"> 1921</a>,\n",
       " <a href=\"/1922/1922.htm\"> 1922</a>,\n",
       " <a href=\"/1923/1923.htm\"> 1923</a>,\n",
       " <a href=\"/1924/1924.htm\"> 1924</a>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# findall links\n",
    "a_tag_list = soup.find_all('a')[:-1]\n",
    "# first few links\n",
    "a_tag_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.planecrashinfo.com//1920/1920.htm',\n",
       " 'http://www.planecrashinfo.com//1921/1921.htm',\n",
       " 'http://www.planecrashinfo.com//1922/1922.htm',\n",
       " 'http://www.planecrashinfo.com//1923/1923.htm',\n",
       " 'http://www.planecrashinfo.com//1924/1924.htm']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract link\n",
    "year_wise_page_link_list = ['http://www.planecrashinfo.com/' + i.get('href') for i in a_tag_list]\n",
    "# first few values\n",
    "year_wise_page_link_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lenght of list\n",
    "len(year_wise_page_link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # container for each crash data\n",
    "# crashes_list_1 = []\n",
    "\n",
    "# # loop through each years page link\n",
    "# for year_wise_page_link in year_wise_page_link_list[:20]:\n",
    "    \n",
    "#     # get contents from the site\n",
    "#     req = requests.get(year_wise_page_link)\n",
    "#     # create a soup object\n",
    "#     soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "#     # find all list inside in that year\n",
    "#     individual_crash_list = soup.find_all('a')[:-1]\n",
    "#     # print(individual_crash_list)\n",
    "    \n",
    "#     # create links\n",
    "#     links = [i.get('href') for i in individual_crash_list]\n",
    "#     # print(links)\n",
    "\n",
    "#     # loop through each crash page link\n",
    "#     for i in links:\n",
    "        \n",
    "#         # get full path\n",
    "#         full_path = year_wise_page_link[:-8] + i\n",
    "#         print(full_path)\n",
    "        \n",
    "#         # save table in a dataframe\n",
    "#         df_crash = pd.read_html(full_path)[0].set_index(0).T\n",
    "        \n",
    "#         # append to crashes_list_1\n",
    "#         crashes_list_1.append(pd.read_html(full_path)[0].set_index(0).T)\n",
    "        \n",
    "# # pickle crashes_list_1      \n",
    "# with open('crashes_list_1', 'wb') as fp:\n",
    "#     pickle.dump(crashes_list_1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # container for each crash data\n",
    "# crashes_list_2 = []\n",
    "\n",
    "# # loop through each years page link\n",
    "# for year_wise_page_link in year_wise_page_link_list[20:40]:\n",
    "    \n",
    "#     # get contents from the site\n",
    "#     req = requests.get(year_wise_page_link)\n",
    "#     # create a soup object\n",
    "#     soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "#     # find all list inside in that year\n",
    "#     individual_crash_list = soup.find_all('a')[:-1]\n",
    "#     # print(individual_crash_list)\n",
    "    \n",
    "#     # create links\n",
    "#     links = [i.get('href') for i in individual_crash_list]\n",
    "#     # print(links)\n",
    "\n",
    "#     # loop through each crash page link\n",
    "#     for i in links:\n",
    "        \n",
    "#         # get full path\n",
    "#         full_path = year_wise_page_link[:-8] + i\n",
    "#         print(full_path)\n",
    "        \n",
    "#         # save table in a dataframe\n",
    "#         df_crash = pd.read_html(full_path)[0].set_index(0).T\n",
    "        \n",
    "#         # append to crashes_list_1\n",
    "#         crashes_list_2.append(pd.read_html(full_path)[0].set_index(0).T)\n",
    "\n",
    "# # pickle crashes_list_2      \n",
    "# with open('crashes_list_2', 'wb') as fp:\n",
    "#     pickle.dump(crashes_list_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # container for each crash data\n",
    "# crashes_list_3 = []\n",
    "\n",
    "# # loop through each years page link\n",
    "# for year_wise_page_link in year_wise_page_link_list[40:60]:\n",
    "    \n",
    "#     # get contents from the site\n",
    "#     req = requests.get(year_wise_page_link)\n",
    "#     # create a soup object\n",
    "#     soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "#     # find all list inside in that year\n",
    "#     individual_crash_list = soup.find_all('a')[:-1]\n",
    "#     # print(individual_crash_list)\n",
    "    \n",
    "#     # create links\n",
    "#     links = [i.get('href') for i in individual_crash_list]\n",
    "#     # print(links)\n",
    "\n",
    "#     # loop through each crash page link\n",
    "#     for i in links:\n",
    "        \n",
    "#         # get full path\n",
    "#         full_path = year_wise_page_link[:-8] + i\n",
    "#         print(full_path)\n",
    "        \n",
    "#         # save table in a dataframe\n",
    "#         df_crash = pd.read_html(full_path)[0].set_index(0).T\n",
    "        \n",
    "#         # append to crashes_list_1\n",
    "#         crashes_list_3.append(pd.read_html(full_path)[0].set_index(0).T)\n",
    "        \n",
    "# # pickle crashes_list_3      \n",
    "# with open('crashes_list_3', 'wb') as fp:\n",
    "#     pickle.dump(crashes_list_3, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # container for each crash data\n",
    "# crashes_list_4 = []\n",
    "\n",
    "# # loop through each years page link\n",
    "# for year_wise_page_link in year_wise_page_link_list[60:80]:\n",
    "    \n",
    "#     # get contents from the site\n",
    "#     req = requests.get(year_wise_page_link)\n",
    "#     # create a soup object\n",
    "#     soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "#     # find all list inside in that year\n",
    "#     individual_crash_list = soup.find_all('a')[:-1]\n",
    "#     # print(individual_crash_list)\n",
    "    \n",
    "#     # create links\n",
    "#     links = [i.get('href') for i in individual_crash_list]\n",
    "#     # print(links)\n",
    "\n",
    "#     # loop through each crash page link\n",
    "#     for i in links:\n",
    "        \n",
    "#         # get full path\n",
    "#         full_path = year_wise_page_link[:-8] + i\n",
    "#         print(full_path)\n",
    "        \n",
    "#         # save table in a dataframe\n",
    "#         df_crash = pd.read_html(full_path)[0].set_index(0).T\n",
    "        \n",
    "#         # append to crashes_list_1\n",
    "#         crashes_list_1.append(pd.read_html(full_path)[0].set_index(0).T)\n",
    "        \n",
    "# # pickle crashes_list_4     \n",
    "# with open('crashes_list_4', 'wb') as fp:\n",
    "#     pickle.dump(crashes_list_4, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # container for each crash data\n",
    "# crashes_list_5 = []\n",
    "\n",
    "# # loop through each years page link\n",
    "# for year_wise_page_link in year_wise_page_link_list[80:]:\n",
    "    \n",
    "#     # get contents from the site\n",
    "#     req = requests.get(year_wise_page_link)\n",
    "#     # create a soup object\n",
    "#     soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "\n",
    "#     # find all list inside in that year\n",
    "#     individual_crash_list = soup.find_all('a')[:-1]\n",
    "#     # print(individual_crash_list)\n",
    "    \n",
    "#     # create links\n",
    "#     links = [i.get('href') for i in individual_crash_list]\n",
    "#     # print(links)\n",
    "\n",
    "#     # loop through each crash page link\n",
    "#     for i in links:\n",
    "        \n",
    "#         # get full path\n",
    "#         full_path = year_wise_page_link[:-8] + i\n",
    "#         print(full_path)\n",
    "        \n",
    "#         # save table in a dataframe\n",
    "#         df_crash = pd.read_html(full_path)[0].set_index(0).T\n",
    "        \n",
    "#         # append to crashes_list_1\n",
    "#         crashes_list_1.append(pd.read_html(full_path)[0].set_index(0).T)\n",
    "        \n",
    "# # pickle crashes_list_5      \n",
    "# with open('crashes_list_5', 'wb') as fp:\n",
    "#     pickle.dump(crashes_list_5, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
